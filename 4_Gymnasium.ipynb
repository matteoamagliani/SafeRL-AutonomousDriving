{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0, Kinematic Model: y = 0.00 / theta = 0.00 / d = 40.00\n",
      "State: 3, Kinematic Model: y = 0.00 / theta = 0.00 / d = 40.00\n",
      "Episode finished after 2 steps with total reward 46.70.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- CMDP Dynamics Functions (from previous code) ---\n",
    "# KinematicModel definition\n",
    "class KinematicModel:\n",
    "    def __init__(self, L, y, theta, Lmax, l, d, p1, sigmaC, p2):\n",
    "        self.L = L\n",
    "        self.y = y\n",
    "        self.theta = theta\n",
    "        self.Lmax = Lmax\n",
    "        self.l = l\n",
    "        self.d = d\n",
    "        self.p1 = p1\n",
    "        self.sigmaC = sigmaC\n",
    "        self.p2 = p2\n",
    "\n",
    "    def input(self, action):\n",
    "        noise = norm.rvs(loc=0, scale=action[0] / 4)\n",
    "        self.y = self.y + action[0] * math.sin(math.radians(action[1] + noise + self.theta))\n",
    "        if random.uniform(0, 1) < self.p2:\n",
    "            self.d = self.d + (self.l - action[0] * math.cos(math.radians(action[1] + noise + self.theta)))\n",
    "        else:\n",
    "            self.d = 40\n",
    "        self.theta = self.theta + (action[0] / self.L) * math.tan(math.radians(action[1] + noise))\n",
    "        if random.uniform(0, 1) < self.p1:\n",
    "            curveAngle = norm.rvs(loc=0, scale=self.sigmaC)\n",
    "            self.theta = self.theta + curveAngle\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"y = {self.y:.2f} / theta = {self.theta:.2f} / d = {self.d:.2f}\"\n",
    "\n",
    "# Transition probability functions (one per outcome)\n",
    "def P1(action, env, sigma, show=True):\n",
    "    ratio = (env.Lmax - env.y) / action[0]\n",
    "    if ratio > 1 or ratio < -1:\n",
    "        term1 = 0\n",
    "    else:\n",
    "        angle = math.degrees(math.asin(ratio))\n",
    "        term1 = 1 - norm.cdf((1 / sigma) * (angle - action[1] - env.theta))\n",
    "    ratio2 = (-env.Lmax - env.y) / action[0]\n",
    "    if ratio2 > 1 or ratio2 < -1:\n",
    "        term2 = 0\n",
    "    else:\n",
    "        angle = math.degrees(math.asin(ratio2))\n",
    "        term2 = norm.cdf((1 / sigma) * (angle - action[1] - env.theta))\n",
    "    value = (1 - env.p1) * (term1 + term2)\n",
    "    return value\n",
    "\n",
    "def P2(action, env, sigma, show=True):\n",
    "    ratio = (env.Lmax - env.y) / action[0]\n",
    "    if ratio > 1 or ratio < -1:\n",
    "        term1 = 0\n",
    "    else:\n",
    "        angle = math.degrees(math.asin(ratio))\n",
    "        term1 = 1 - norm.cdf((1 / sigma) * (angle - action[1] - env.theta))\n",
    "    ratio2 = (-env.Lmax - env.y) / action[0]\n",
    "    if ratio2 > 1 or ratio2 < -1:\n",
    "        term2 = 0\n",
    "    else:\n",
    "        angle = math.degrees(math.asin(ratio2))\n",
    "        term2 = norm.cdf((1 / sigma) * (angle - action[1] - env.theta))\n",
    "    value = env.p1 * (term1 + term2)\n",
    "    return value\n",
    "\n",
    "def P3(action, env, sigma, show=True):\n",
    "    if (1 / action[0]) * (env.l + (((action[0] * 3.6) / -2) + env.d)) > 1:\n",
    "        return 0\n",
    "    angle = math.degrees(math.acos((1 / action[0]) * (env.l + (((action[0] * 3.6) / -2) + env.d))))\n",
    "    value = norm.cdf((1 / sigma) * (angle - abs(action[1]) - abs(env.theta)))\n",
    "    return env.p2 * (1 - env.p1) * value\n",
    "\n",
    "def P4(action, env, sigma, show=True):\n",
    "    if (1 / action[0]) * (env.l + (((action[0] * 3.6) / -2) + env.d)) > 1:\n",
    "        return 0\n",
    "    angle = math.degrees(math.acos((1 / action[0]) * (env.l + (((action[0] * 3.6) / -2) + env.d))))\n",
    "    value = norm.cdf((1 / sigma) * (angle - abs(action[1]) - abs(env.theta)))\n",
    "    return env.p2 * env.p1 * value\n",
    "\n",
    "# Combined transition functions to yield probability of each outcome:\n",
    "def anyToG(action, env):\n",
    "    p1 = P1(action, env, action[0] / 4, show=False)\n",
    "    p2 = P2(action, env, math.sqrt(action[0] / 4 + env.sigmaC), show=False)\n",
    "    term1 = 1 - (p1 + p2)\n",
    "    term2 = 1 - (P3(action, env, action[0] / 4, show=False) + \n",
    "                 P4(action, env, math.sqrt(action[0] / 4 + env.sigmaC), show=False))\n",
    "    return term1 * term2\n",
    "\n",
    "def anyToX(action, env):\n",
    "    p1 = P1(action, env, action[0] / 4, show=False)\n",
    "    p2 = P2(action, env, math.sqrt(action[0] / 4 + env.sigmaC), show=False)\n",
    "    term1 = p1 + p2\n",
    "    term2 = 1 - (P3(action, env, action[0] / 4, show=False) + \n",
    "                 P4(action, env, math.sqrt(action[0] / 4 + env.sigmaC), show=False))\n",
    "    return term1 * term2\n",
    "\n",
    "def anyToI(action, env):\n",
    "    p1 = P1(action, env, action[0] / 4, show=False)\n",
    "    p2 = P2(action, env, math.sqrt(action[0] / 4 + env.sigmaC), show=False)\n",
    "    term1 = 1 - (p1 + p2)\n",
    "    term2 = (P3(action, env, action[0] / 4, show=False) + \n",
    "             P4(action, env, math.sqrt(action[0] / 4 + env.sigmaC), show=False))\n",
    "    return term1 * term2\n",
    "\n",
    "def anyToXI(action, env):\n",
    "    p1 = P1(action, env, action[0] / 4, show=False)\n",
    "    p2 = P2(action, env, math.sqrt(action[0] / 4 + env.sigmaC), show=False)\n",
    "    term1 = p1 + p2\n",
    "    term2 = (P3(action, env, action[0] / 4, show=False) + \n",
    "             P4(action, env, math.sqrt(action[0] / 4 + env.sigmaC), show=False))\n",
    "    return term1 * term2\n",
    "\n",
    "# Expected reward function\n",
    "def rewardCenterProbability(action, env, ratio, show=True):\n",
    "    numerator1 = (env.Lmax * ratio) - env.y\n",
    "    if abs(numerator1 / action[0]) > 1:\n",
    "        term1 = term1C = 0\n",
    "    else:\n",
    "        angle = math.degrees(math.asin(numerator1 / action[0]))\n",
    "        term1 = norm.cdf((4 / action[0]) * (angle - action[1] - env.theta))\n",
    "        term1C = norm.cdf((1 / math.sqrt(action[0] / 4 + env.sigmaC)) * (angle - action[1] - env.theta))\n",
    "    numerator2 = (-env.Lmax * ratio) - env.y\n",
    "    if abs(numerator2 / action[0]) > 1:\n",
    "        term2 = term2C = 0\n",
    "    else:\n",
    "        angle = math.degrees(math.asin(numerator2 / action[0]))\n",
    "        term2 = norm.cdf((4 / action[0]) * (angle - action[1] - env.theta))\n",
    "        term2C = norm.cdf((1 / math.sqrt(action[0] / 4 + env.sigmaC)) * (angle - action[1] - env.theta))\n",
    "    return ((1 - env.p1) * (term1 - term2)) + (env.p1 * (term1C - term2C))\n",
    "\n",
    "def rewardSpeed(action, env):\n",
    "    return -0.5 * abs(env.l - action[0])\n",
    "\n",
    "def rewardDistanceProbability(action, env):\n",
    "    term1 = P3(action, env, action[0] / 4, show=False)\n",
    "    term2 = P4(action, env, math.sqrt(action[0] / 4 + env.sigmaC), show=False)\n",
    "    return term1 + term2\n",
    "\n",
    "def expectedReward(action, env, r1, r2, r3, r4, show=True):\n",
    "    termR1 = r1 * rewardCenterProbability(action, env, 0.5, show=False)\n",
    "    termR2 = r2 * rewardCenterProbability(action, env, 0.25, show=False)\n",
    "    rSpeed = rewardSpeed(action, env)\n",
    "    termR3 = r3 * (1 - rewardDistanceProbability(action, env))\n",
    "    termR4 = r4 * rewardDistanceProbability(action, env)\n",
    "    return termR1 + termR2 + rSpeed + termR3 + termR4\n",
    "\n",
    "# --- Custom Gym Environment using the CMDP ---\n",
    "class KinematicEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, kinematic_model):\n",
    "        super(KinematicEnv, self).__init__()\n",
    "        self.kinematic_model = kinematic_model\n",
    "        \n",
    "        # Define observation: discrete state (0: G, 1: X, 2: I, 3: XI)\n",
    "        self.observation_space = spaces.Discrete(4)\n",
    "        # Define action space: a finite set of actions (velocity, steering angle)\n",
    "        self.actions = [(v, delta) for v in [20, 25, 30] for delta in [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]]\n",
    "        self.action_space = spaces.Discrete(len(self.actions))\n",
    "        \n",
    "        # Start in a \"Good\" state by default\n",
    "        self.state = 0\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "        # Reset the kinematic model parameters\n",
    "        self.kinematic_model.y = 0\n",
    "        self.kinematic_model.theta = 0\n",
    "        self.kinematic_model.d = 40\n",
    "        self.state = 0  # Good state (G)\n",
    "        return self.state, {}\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        action = self.actions[action_index]\n",
    "        # Compute transition probabilities for each state\n",
    "        probs = np.array([\n",
    "            anyToG(action, self.kinematic_model),\n",
    "            anyToX(action, self.kinematic_model),\n",
    "            anyToI(action, self.kinematic_model),\n",
    "            anyToXI(action, self.kinematic_model)\n",
    "        ])\n",
    "        if probs.sum() == 0:\n",
    "            probs = np.array([1.0, 0, 0, 0])\n",
    "        else:\n",
    "            probs = probs / probs.sum()\n",
    "        # Sample the next state from the discrete outcomes {0:G, 1:X, 2:I, 3:XI}\n",
    "        next_state = int(np.random.choice([0, 1, 2, 3], p=probs))\n",
    "        # Calculate reward using our expectedReward function\n",
    "        r1, r2, r3, r4 = 50, 100, -1, 0\n",
    "        reward = expectedReward(action, self.kinematic_model, r1, r2, r3, r4, show=False)\n",
    "        self.state = next_state\n",
    "        # Mark episode as done if state is unsafe (X or XI)\n",
    "        done = self.state in [1, 3]\n",
    "        return self.state, reward, done, False, {}\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        print(f\"State: {self.state}, Kinematic Model: {self.kinematic_model}\")\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# --- Testing the Environment with a Random Policy ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Instantiate a kinematic model with fixed parameters\n",
    "    km = KinematicModel(L=1, y=0, theta=0, Lmax=2, l=25, d=40, p1=0.05, sigmaC=5, p2=0.3)\n",
    "    env = KinematicEnv(km)\n",
    "    obs, _ = env.reset(seed=42)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done and step_count < 20:\n",
    "        # For testing, choose a random action\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "    \n",
    "    print(f\"Episode finished after {step_count} steps with total reward {total_reward:.2f}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (y, theta) \u001b[38;5;129;01min\u001b[39;00m initial_conditions:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Create a new kinematic model instance for each initial condition\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     km \u001b[38;5;241m=\u001b[39m KinematicModel(L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, y\u001b[38;5;241m=\u001b[39my, theta\u001b[38;5;241m=\u001b[39mtheta, Lmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, l\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, p1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, sigmaC\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, p2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m     V, policy \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     results[(y, theta)] \u001b[38;5;241m=\u001b[39m (V, policy)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial condition y = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, theta = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m, in \u001b[0;36mvalue_iteration\u001b[1;34m(env, gamma, theta_thresh)\u001b[0m\n\u001b[0;32m     49\u001b[0m     exp_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m s_prime, p \u001b[38;5;129;01min\u001b[39;00m transition_func(a, env):\n\u001b[1;32m---> 51\u001b[0m         exp_value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p \u001b[38;5;241m*\u001b[39m (\u001b[43mreward_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr4\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m V[s_prime])\n\u001b[0;32m     52\u001b[0m     action_values[a] \u001b[38;5;241m=\u001b[39m exp_value\n\u001b[0;32m     53\u001b[0m best_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(action_values, key\u001b[38;5;241m=\u001b[39maction_values\u001b[38;5;241m.\u001b[39mget)\n",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m, in \u001b[0;36mreward_func\u001b[1;34m(a, env, r1, r2, r3, r4)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreward_func\u001b[39m(a, env, r1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, r2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, r3\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, r4\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexpectedReward\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 144\u001b[0m, in \u001b[0;36mexpectedReward\u001b[1;34m(action, env, r1, r2, r3, r4, show)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexpectedReward\u001b[39m(action, env, r1, r2, r3, r4, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 144\u001b[0m     termR1 \u001b[38;5;241m=\u001b[39m r1 \u001b[38;5;241m*\u001b[39m \u001b[43mrewardCenterProbability\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     termR2 \u001b[38;5;241m=\u001b[39m r2 \u001b[38;5;241m*\u001b[39m rewardCenterProbability(action, env, \u001b[38;5;241m0.25\u001b[39m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    146\u001b[0m     rSpeed \u001b[38;5;241m=\u001b[39m rewardSpeed(action, env)\n",
      "Cell \u001b[1;32mIn[1], line 125\u001b[0m, in \u001b[0;36mrewardCenterProbability\u001b[1;34m(action, env, ratio, show)\u001b[0m\n\u001b[0;32m    123\u001b[0m     angle \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mdegrees(math\u001b[38;5;241m.\u001b[39masin(numerator1 \u001b[38;5;241m/\u001b[39m action[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m    124\u001b[0m     term1 \u001b[38;5;241m=\u001b[39m norm\u001b[38;5;241m.\u001b[39mcdf((\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m/\u001b[39m action[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m (angle \u001b[38;5;241m-\u001b[39m action[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m env\u001b[38;5;241m.\u001b[39mtheta))\n\u001b[1;32m--> 125\u001b[0m     term1C \u001b[38;5;241m=\u001b[39m \u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmaC\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mangle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m numerator2 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39menv\u001b[38;5;241m.\u001b[39mLmax \u001b[38;5;241m*\u001b[39m ratio) \u001b[38;5;241m-\u001b[39m env\u001b[38;5;241m.\u001b[39my\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(numerator2 \u001b[38;5;241m/\u001b[39m action[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Matteo Amagliani\\.conda\\envs\\safeRL_gym\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:2081\u001b[0m, in \u001b[0;36mrv_continuous.cdf\u001b[1;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(cond):  \u001b[38;5;66;03m# call only if at least 1 entry\u001b[39;00m\n\u001b[0;32m   2080\u001b[0m     goodargs \u001b[38;5;241m=\u001b[39m argsreduce(cond, \u001b[38;5;241m*\u001b[39m((x,)\u001b[38;5;241m+\u001b[39margs))\n\u001b[1;32m-> 2081\u001b[0m     \u001b[43mplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgoodargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[()]\n",
      "File \u001b[1;32mc:\\Users\\Matteo Amagliani\\.conda\\envs\\safeRL_gym\\lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1957\u001b[0m, in \u001b[0;36mplace\u001b[1;34m(arr, mask, vals)\u001b[0m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_place_dispatcher)\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplace\u001b[39m(arr, mask, vals):\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;124;03m    Change elements of an array based on conditional and input values.\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1955\u001b[0m \n\u001b[0;32m   1956\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_place\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "# (Reuse the same definitions for KinematicModel, anyToG, anyToX, anyToI, anyToXI, expectedReward, etc.)\n",
    "# For brevity, we assume the same functions defined above (P1, P2, P3, P4, reward functions, etc.) are already present.\n",
    "\n",
    "# Define states and actions (as before)\n",
    "states = ['G', 'X', 'I', 'XI']\n",
    "anglesDeg = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "velocities = [20, 25, 30]\n",
    "actions = [(v, delta) for v in velocities for delta in anglesDeg]\n",
    "\n",
    "def transition_func(a, env):\n",
    "    # For a given action, return a list of (state, probability) pairs.\n",
    "    probabilities = [\n",
    "        anyToG(a, env),\n",
    "        anyToX(a, env),\n",
    "        anyToI(a, env),\n",
    "        anyToXI(a, env)\n",
    "    ]\n",
    "    # Normalize probabilities if needed\n",
    "    s = sum(probabilities)\n",
    "    if s > 0:\n",
    "        probabilities = [p/s for p in probabilities]\n",
    "    else:\n",
    "        probabilities = [1.0, 0, 0, 0]\n",
    "    return list(zip(states, probabilities))\n",
    "\n",
    "def reward_func(a, env, r1=50, r2=100, r3=-1, r4=0):\n",
    "    return expectedReward(a, env, r1, r2, r3, r4, show=False)\n",
    "\n",
    "def value_iteration(env, gamma=0.9, theta_thresh=1e-4):\n",
    "    # Initialize value function for each state\n",
    "    V = {s: 0 for s in states}\n",
    "    policy = {s: None for s in states}\n",
    "    r1, r2, r3, r4 = 50, 100, -1, 0\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v_old = V[s]\n",
    "            action_values = {}\n",
    "            for a in actions:\n",
    "                # For a stationary CMDP model the transition probabilities and rewards\n",
    "                # do not depend on s, so we simply compute the expected value.\n",
    "                exp_value = 0\n",
    "                for s_prime, p in transition_func(a, env):\n",
    "                    exp_value += p * (reward_func(a, env, r1, r2, r3, r4) + gamma * V[s_prime])\n",
    "                action_values[a] = exp_value\n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            V[s] = action_values[best_action]\n",
    "            policy[s] = best_action\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "        if delta < theta_thresh:\n",
    "            break\n",
    "        iteration += 1\n",
    "    return V, policy\n",
    "\n",
    "# Define a grid of initial conditions (varying lateral offset and heading)\n",
    "initial_conditions = [(y, theta) for y in np.linspace(-1, 1, 5) for theta in np.linspace(-5, 5, 5)]\n",
    "results = {}\n",
    "\n",
    "for (y, theta) in initial_conditions:\n",
    "    # Create a new kinematic model instance for each initial condition\n",
    "    km = KinematicModel(L=1, y=y, theta=theta, Lmax=2, l=25, d=40, p1=0.05, sigmaC=5, p2=0.3)\n",
    "    V, policy = value_iteration(km, gamma=0.9, theta_thresh=1e-4)\n",
    "    results[(y, theta)] = (V, policy)\n",
    "    print(f\"Initial condition y = {y:.2f}, theta = {theta:.2f}\")\n",
    "    print(\"Value Function:\", V)\n",
    "    print(\"Policy:\", policy)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prove safetyGym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import safety_gymnasium\n",
    "\n",
    "env = safety_gymnasium.make('SafetyPointCircle0-v0', render_mode='human')\n",
    "'''\n",
    "Vision Environment\n",
    "    env = safety_gymnasium.make('SafetyPointCircle0Vision-v0', render_mode='human')\n",
    "Keyboard Debug environment\n",
    "due to the complexity of the agent's inherent dynamics, only partial support for the agent.\n",
    "    env = safety_gymnasium.make('SafetyPointCircle0Debug-v0', render_mode='human')\n",
    "'''\n",
    "obs, info = env.reset()\n",
    "# Set seeds\n",
    "# obs, _ = env.reset(seed=0)\n",
    "terminated, truncated = False, False\n",
    "ep_ret, ep_cost = 0, 0\n",
    "for _ in range(1000):\n",
    "    assert env.observation_space.contains(obs)\n",
    "    act = env.action_space.sample()\n",
    "    assert env.action_space.contains(act)\n",
    "    # modified for Safe RL, added cost\n",
    "    obs, reward, cost, terminated, truncated, info = env.step(act)\n",
    "    ep_ret += reward\n",
    "    ep_cost += cost\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Environment Safexp-RaceCarGoal1-v2 is not registered in safety-gymnasium.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Part 1: Single-Episode Simulation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# \"Safexp-RaceCarGoal1-v0\" is a pre-built task where the agent is a racecar.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# In this configuration the environment includes Sigwalls that act as road boundaries.\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43msafety_gymnasium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSafexp-RaceCarGoal1-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Reset the environment (optionally specify a seed for reproducibility)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Matteo Amagliani\\.conda\\envs\\safeRL_gym\\lib\\site-packages\\safety_gymnasium\\utils\\registration.py:91\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# For string id's, load the environment spec from the registry then make the environment spec\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m safe_registry, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnvironment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not registered in safety-gymnasium.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     env_spec \u001b[38;5;241m=\u001b[39m _find_spec(\u001b[38;5;28mid\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Environment Safexp-RaceCarGoal1-v2 is not registered in safety-gymnasium."
     ]
    }
   ],
   "source": [
    "import safety_gymnasium  # the Safety Gymnasium package\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Part 1: Single-Episode Simulation\n",
    "# -------------------------------\n",
    "\n",
    "# Create the environment.\n",
    "# \"Safexp-RaceCarGoal1-v0\" is a pre-built task where the agent is a racecar.\n",
    "# In this configuration the environment includes Sigwalls that act as road boundaries.\n",
    "env = safety_gymnasium.make(\"Safexp-RaceCarGoal1-v2\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment (optionally specify a seed for reproducibility)\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "\n",
    "print(\"Starting single-episode simulation...\")\n",
    "while not done:\n",
    "    # For demonstration, we sample a random action.\n",
    "    # In practice, you could use a learned policy.\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Render the environment (the \"racecar\" along a straight road with sigwalls)\n",
    "    env.render()\n",
    "    time.sleep(0.05)  # slow down the simulation for visualization\n",
    "\n",
    "print(f\"Episode finished with total reward: {total_reward:.2f}\")\n",
    "env.close()\n",
    "\n",
    "# -------------------------------\n",
    "# Part 2: Multiple Episodes with Varying Initial Conditions\n",
    "# -------------------------------\n",
    "\n",
    "# In Safety Gymnasium, initial conditions are typically randomized.\n",
    "# We can simulate multiple episodes by resetting with different seeds.\n",
    "num_episodes = 5\n",
    "\n",
    "print(\"\\nStarting multiple-episode simulation with varying seeds:\")\n",
    "for seed in range(100, 100 + num_episodes):\n",
    "    env = safety_gymnasium.make(\"Safexp-RaceCarGoal1-v0\", render_mode=\"human\")\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    \n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "        steps += 1\n",
    "        if steps > 200:  # safety exit condition for long episodes\n",
    "            break\n",
    "            \n",
    "    print(f\"Episode with seed {seed} finished with reward: {episode_reward:.2f} in {steps} steps.\")\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.], [inf inf inf inf inf inf inf inf inf inf inf inf  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.], (28,), float64)\n",
      "Action space: Box([-20.          -0.78500003], [20.          0.78500003], (2,), float64)\n",
      "Starting single-episode simulation of the Racecar environment...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Step the environment using the sampled action.\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     37\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     38\u001b[0m step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import safety_gymnasium\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Single-Episode Simulation of a Racecar Environment\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Try to create the Racecar environment with sigwalls (representing road boundaries).\n",
    "# According to the documentation, the Racecar agent uses realistic dynamics and has\n",
    "# an action space defined as a Box with limits on rear-wheel velocity and front-wheel steering.\n",
    "# If the environment \"Safexp-RaceCarGoal1-v2\" is not available, you may need to try a different ID.\n",
    "#env_id = \"Safexp-RaceCarGoal1-v2\"\n",
    "env_id= \"SafetyRacecarCircle1-v0\"\n",
    "try:\n",
    "    env = safety_gymnasium.make(env_id, render_mode=\"human\")\n",
    "except AssertionError as e:\n",
    "    print(f\"Environment {env_id} is not registered in Safety Gymnasium. Please check available environment IDs.\")\n",
    "    exit()\n",
    "\n",
    "# Reset the environment with a fixed seed (for reproducibility)\n",
    "obs, info = env.reset(seed=42)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "step_count = 0\n",
    "\n",
    "print(\"Starting single-episode simulation of the Racecar environment...\")\n",
    "while not done:\n",
    "    # For testing, we choose a random action.\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step the environment using the sampled action.\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    step_count += 1\n",
    "    \n",
    "    # Render the simulation (the Racecar, sigwalls, and other static objects)\n",
    "    env.render()\n",
    "    time.sleep(0.05)  # Slow down the simulation for visualization\n",
    "    \n",
    "    # Optional exit condition: terminate after a fixed number of steps to prevent endless episodes.\n",
    "    if step_count >= 300:\n",
    "        break\n",
    "\n",
    "print(f\"Episode finished after {step_count} steps with total reward: {total_reward:.2f}\")\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
