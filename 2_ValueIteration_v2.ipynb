{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8bf29b5-b8d7-414c-9a3b-1bcda1aaddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a300826f-563e-40d7-b9a7-d2f6af92bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KinematicModel:\n",
    "\n",
    "    def __init__(self, L, y, theta, Lmax, l, d, p1, sigmaC, p2):\n",
    "        self.L = L\n",
    "        self.y = y\n",
    "        self.theta = theta\n",
    "        self.Lmax = Lmax\n",
    "        self.l = l\n",
    "        self.d = d\n",
    "        self.p1 = p1\n",
    "        self.sigmaC = sigmaC\n",
    "        self.p2 = p2\n",
    "\n",
    "    def input(self, action):\n",
    "        noise = norm.rvs(loc=0, scale=action[0] / 4)\n",
    "        self.y = self.y + action[0] * math.sin(math.radians(action[1] + noise + self.theta))\n",
    "        nextVehicleEvent = random.uniform(0, 1)\n",
    "        if nextVehicleEvent < self.p2:\n",
    "            self.d = self.d + (self.l - action[0] * math.cos(math.radians(action[1] + noise + self.theta)))\n",
    "        else:\n",
    "            self.d = 40\n",
    "        self.theta = self.theta + (action[0] / self.L) * math.tan(math.radians(action[1] + noise))\n",
    "        curveEvent = random.uniform(0, 1)\n",
    "        if curveEvent < self.p1:\n",
    "            curveAngle = norm.rvs(loc=0, scale=self.sigmaC)\n",
    "            self.theta = self.theta + curveAngle\n",
    "            \n",
    "            \n",
    "    def __str__(self):\n",
    "        return f\"y = {self.y} / theta = {self.theta} / d = {self.d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1013600-cd85-4135-aa96-dd828fc07f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of going out of a straight road given an action \n",
    "def P1(action, environment, sigma, show = True):\n",
    "    if (environment.Lmax - environment.y) / action[0] > 1 or (environment.Lmax - environment.y) / action[0] < -1:\n",
    "        term1 = 0\n",
    "    # Term 1 is the probability that y > Lmax\n",
    "    else:\n",
    "        term1 = math.degrees(math.asin((environment.Lmax - environment.y) / action[0]))\n",
    "        term1 = 1 - norm.cdf((1 / sigma) * (term1 - action[1] - environment.theta))\n",
    "    #print(f\"Term 1: {term1}\")\n",
    "    # Term 2 is the probability that y < -Lmax\n",
    "    if (- environment.Lmax - environment.y) / action[0] > 1 or (- environment.Lmax - environment.y) / action[0] < -1:\n",
    "        term2 = 0\n",
    "    else:\n",
    "        term2 = math.degrees(math.asin((- environment.Lmax - environment.y) / action[0]))\n",
    "        term2 = norm.cdf((1 / sigma) * (term2 - action[1] - environment.theta))\n",
    "    #print(f\"Term 2: {term2}\")\n",
    "    value = (1 - environment.p1) * (term1 + term2)\n",
    "    if show:\n",
    "        print(f\"Value: {value}\")\n",
    "    return value, term1, term2\n",
    "    \n",
    "# Probability of going out of a curved road given an action\n",
    "def P2(action, environment, sigma, show = True):\n",
    "    # Term 1 is the probability that y > Lmax\n",
    "    if (environment.Lmax - environment.y) / action[0] > 1 or (environment.Lmax - environment.y) / action[0] < -1:\n",
    "        term1 = 0\n",
    "    else:\n",
    "        term1 = math.degrees(math.asin((environment.Lmax - environment.y) / action[0]))\n",
    "        term1 = 1 - norm.cdf((1 / sigma) * (term1 - action[1] - environment.theta))\n",
    "    #print(f\"Term 1: {term1}\")\n",
    "    # Term 2 is the probability that y < -Lmax\n",
    "    if (- environment.Lmax - environment.y) / action[0] > 1 or (- environment.Lmax - environment.y) / action[0] < -1:\n",
    "        term2 = 0\n",
    "    else:\n",
    "        term2 = math.degrees(math.asin((- environment.Lmax - environment.y) / action[0]))\n",
    "        term2 = norm.cdf((1 / sigma) * (term2 - action[1] - environment.theta))\n",
    "    #print(f\"Term 2: {term2}\")\n",
    "    value = environment.p1 * (term1 + term2)\n",
    "    if show:\n",
    "        print(f\"Value: {value}\")\n",
    "    return value, term1, term2\n",
    "\n",
    "# Probability of risking a crash in a straight road given an action \n",
    "def P3(action, environment, sigma, show = True):\n",
    "    if (1 / action[0]) * (environment.l + ( ((action[0] * 3.6) / -2) + environment.d)) > 1:\n",
    "        return 0\n",
    "    value = math.degrees(math.acos((1 / action[0]) * (environment.l + (((action[0] * 3.6) / -2) + environment.d))))\n",
    "    value = norm.cdf((1 / sigma) * (value - math.fabs(action[1]) - math.fabs(environment.theta)))\n",
    "    value = environment.p2 * (1 - environment.p1) * value\n",
    "    if show:\n",
    "        print(f\"Value: {value}\")\n",
    "    return value\n",
    "\n",
    "# Probability of risking a crash in a curved road given an action\n",
    "def P4(action, environment, sigma, show = True):\n",
    "    if (1 / action[0]) * (environment.l + ( ((action[0] * 3.6) / -2) + environment.d)) > 1:\n",
    "        return 0\n",
    "    value = math.degrees(math.acos((1 / action[0]) * (environment.l + (((action[0] * 3.6) / -2) + environment.d))))\n",
    "    value = norm.cdf((1 / sigma) * (value - math.fabs(action[1]) - math.fabs(environment.theta)))\n",
    "    value = environment.p2 * environment.p1 * value\n",
    "    if show:\n",
    "        print(f\"Value: {value}\")\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d928e2-c611-4be9-8d8b-2494aac6333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition probabilities\n",
    "\n",
    "def anyToG(action, environment):\n",
    "    P1_val, _, _ = P1(action, environment, action[0]/4, False)\n",
    "    P2_val, _, _ = P2(action, environment, math.sqrt(action[0]/4 + environment.sigmaC), False)\n",
    "    # Term 1 is the probability of NOT going out of the road\n",
    "    term1 = 1 - (P1_val + P2_val)\n",
    "    # Term 2 is the probability of NOT risking a crash\n",
    "    term2 = 1 - (P3(action, environment, action[0]/4, False) + P4(action, environment, math.sqrt(action[0]/4 + environment.sigmaC), False))\n",
    "    return term1 * term2\n",
    "    \n",
    "def anyToX(action, environment):\n",
    "    P1_val, _, _ = P1(action, environment, action[0]/4, False)\n",
    "    P2_val, _, _ = P2(action, environment, math.sqrt(action[0]/4 + environment.sigmaC), False)\n",
    "    # Term 1 is the probability of going out of the road\n",
    "    term1 = P1_val + P2_val\n",
    "    # Term 2 is the probability of NOT risking a crash\n",
    "    term2 = 1 - (P3(action, environment, action[0]/4, False) + P4(action, environment, math.sqrt(action[0]/4 + environment.sigmaC), False))\n",
    "    return term1 * term2\n",
    "\n",
    "def anyToI(action, environment):\n",
    "    P1_val, _, _ = P1(action, environment, action[0]/4, False)\n",
    "    P2_val, _, _ = P2(action, environment, math.sqrt(action[0]/4 + environment.sigmaC), False)\n",
    "    # Term 1 is the probability of NOT going out of the road\n",
    "    term1 = 1 - (P1_val + P2_val)\n",
    "    # Term 2 is the probability of risking a crash\n",
    "    term2 = P3(action, environment, action[0]/4, False) + P4(action, environment, math.sqrt(action[0]/4 + environment.sigmaC), False)\n",
    "    return term1 * term2\n",
    "\n",
    "def anyToXI(action, environment):\n",
    "    P1_val, _, _ = P1(action, environment, action[0]/4, False)\n",
    "    P2_val, _, _ = P2(action, environment, math.sqrt(action[0]/4 + environment.sigmaC), False)\n",
    "    # Term 1 is the probability of going out of the road\n",
    "    term1 = P1_val + P2_val\n",
    "    # Term 2 is the probability of risking a crash\n",
    "    term2 = P3(action, environment, action[0]/4, False) + P4(action, environment, math.sqrt(action[0]/4 + environment.sigmaC), False)\n",
    "    return term1 * term2\n",
    "\n",
    "def transitionProbabilitiesMatrix(actions, environment):\n",
    "    transitions = [anyToG, anyToX, anyToI, anyToXI]\n",
    "    transitionMatrix = np.array([[transition(action, kinematicModel) for transition in transitions] for action in actions])\n",
    "    return transitionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf0f8f90-2cde-4cf5-970a-87cfc1490133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward for staying at the center of the road\n",
    "def rewardCenterProbability(action, environment, ratio, show = True):\n",
    "    # Term 1 is the probability that y < Lmax * ratio\n",
    "    if ((environment.Lmax * ratio) - environment.y) / action[0] > 1 or ((environment.Lmax * ratio) - environment.y) / action[0] < -1:\n",
    "        term1 = 0\n",
    "        term1C = 0\n",
    "    else:\n",
    "        term1 = math.degrees(math.asin(((environment.Lmax * ratio) - environment.y) / action[0]))\n",
    "        term1C = norm.cdf((1 / (math.sqrt(action[0]/4 + environment.sigmaC))) * (term1 - action[1] - environment.theta))\n",
    "        term1 = norm.cdf((1 / (action[0]/4)) * (term1 - action[1] - environment.theta))\n",
    "    #print(f\"Term 1: {term1}\")\n",
    "    # Term 2 is the probability that y > -Lmax * ratio\n",
    "    if ((-environment.Lmax * ratio) - environment.y) / action[0] > 1 or ((-environment.Lmax * ratio) - environment.y) / action[0] < -1:\n",
    "        term2 = 0\n",
    "        term2C = 0\n",
    "    else:\n",
    "        term2 = math.degrees(math.asin(((-environment.Lmax * ratio) - environment.y) / action[0]))\n",
    "        term2C = norm.cdf((1 / (math.sqrt(action[0]/4 + environment.sigmaC))) * (term2 - action[1] - environment.theta))\n",
    "        term2 = norm.cdf((1 / (action[0]/4)) * (term2 - action[1] - environment.theta))\n",
    "    #print(f\"Term 2: {term2}\")\n",
    "    value = ((1 - environment.p1) * (term1 - term2)) + (environment.p1 * (term1C - term2C))\n",
    "    if show:\n",
    "        print(f\"Value: {value}\")\n",
    "    return value\n",
    "\n",
    "# Reward for going at the correct speed\n",
    "def rewardSpeed(action, environment):\n",
    "    return -0.5 * (math.fabs(environment.l - action[0]))\n",
    "\n",
    "# Reward for staying at the correct distance\n",
    "def rewardDistanceProbability(action, environment):\n",
    "    # Probability of risking a crash in a straight road given an action \n",
    "    term1 = P3(action, environment, action[0] / 4, False)\n",
    "    #Probability of risking a crash in a curved road given an action\n",
    "    term2 = P4(action, environment, math.sqrt(action[0]/4 + environment.sigmaC), False)\n",
    "    return term1 + term2\n",
    "\n",
    "def rewardOutOfRoadProbability(action, environment):\n",
    "    # Probability of going out of a straight road given an action\n",
    "    term1, _, _ = P1(action, environment, action[0]/4, False)\n",
    "    # Probability of going out of a curved road given an action\n",
    "    term2, _, _ = P2(action, environment, action[0]/4, False)\n",
    "    return term1 + term2\n",
    "\n",
    "# Expected reward given an action\n",
    "def expectedReward(action, environment, r1, r2, r3, r4, show = True):\n",
    "    termR1 = r1 * rewardCenterProbability(action, environment, 0.5, False)\n",
    "    #print(f\"Term r1: {termR1}\")\n",
    "    termR2 = r2 * rewardCenterProbability(action, environment, 0.25, False)\n",
    "    #print(f\"Term r2: {termR2}\")\n",
    "    rSpeed = rewardSpeed(action, environment)\n",
    "    if show:\n",
    "        print(f\"Reward for action [{action[0]}, {action[1]}]: {round(termR1 + termR2 + rSpeed, 2)}\")\n",
    "    return termR1 + termR2 + rSpeed\n",
    "\n",
    "def expectedRewardMatrix(action, environment, r1, r2, r3, r4):\n",
    "    rewardMatrix = np.array([expectedReward(action, kinematicModel, r1, r2, r3, r4, False)for action in actions])\n",
    "    return rewardMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3560319f-2730-4ef3-9893-b1d7dace4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, -5)\n"
     ]
    }
   ],
   "source": [
    "# Environment starts:\n",
    "# - At center of the road y = 0 m\n",
    "# - Aligned with the road theta = 0°\n",
    "# - Road width 4 m Lmax = 2 m\n",
    "# - Road limit 90 km/h l = 25 m/s\n",
    "# - Distance from next vehicle d = 40 m\n",
    "# - Probability of a road curve p1 = 5%\n",
    "# - Variance of the curve sigmaC = 5\n",
    "# - Probability of a vehicle in front p2 = 30%\n",
    "kinematicModel = KinematicModel(1, 0, -5, 2, 25, 40, 0.05, 5, 0.3)\n",
    "\n",
    "# State space\n",
    "# 0 = S_G: good state\n",
    "# 1 = S_X: out of road state\n",
    "# 2 = S_I: risk of crash state\n",
    "# 3 = S_XI: out of road and risk of crash state\n",
    "states = ['G', 'X', 'I', 'XI']\n",
    "\n",
    "# Action space\n",
    "anglesDeg = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "velocities = [20, 25, 30]\n",
    "actions = [(v, delta) for v in velocities for delta in anglesDeg]\n",
    "#print(f\"{actions}\")\n",
    "\n",
    "# State-action transition probability matrix\n",
    "#T = transitionProbabilitiesMatrix(actions, kinematicModel)\n",
    "#print(f\"{T}\")\n",
    "\n",
    "# State-action expected reward matrix\n",
    "r1 = 50\n",
    "r2 = 100\n",
    "r3 = -1\n",
    "r4 = 0\n",
    "#R = expectedRewardMatrix(actions, kinematicModel, r1, r2, r3, r4)\n",
    "#print(f\"{R}\")\n",
    "\n",
    "print(f\"{actions[0]}\")\n",
    "\n",
    "# Transition function\n",
    "def transition_func(s, a, T):\n",
    "    action_index = actions.index(a)\n",
    "    # Seleziona la riga corrispondente nella matrice di transizione\n",
    "    probabilities = T[action_index]\n",
    "    # Crea la lista di tuple (stato, probabilità) usando lo zip tra 'states' e le probabilità\n",
    "    transitions = [(state, prob) for state, prob in zip(states, probabilities)]\n",
    "    return transitions\n",
    "\n",
    "#print(f\"{transition_func('G', (20, -30), T)}\")\n",
    "\n",
    "# Reward function (NON SAFE)\n",
    "def reward_func(s, a, s_prime, R):\n",
    "    action_index = actions.index(a)\n",
    "    return R[action_index]\n",
    "    \n",
    "# Reward function (SAFE)\n",
    "def reward_func_SAFE(s, a, s_prime, R):\n",
    "    cost = 0\n",
    "    action_index = actions.index(a)\n",
    "    if s_prime == 'X' or s_prime == 'XI':\n",
    "        cost -= 50\n",
    "    elif s_prime == 'I' or s_prime == 'XI':\n",
    "        cost -= 50\n",
    "    return R[action_index] + cost\n",
    "\n",
    "#print(f\"{reward_func('G', (20, -30), 'G', R)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb7d149a-7232-4136-ba5d-563c9a8afb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 0 ---\n",
      "State G: Best action (25, 5) with value 3.00\n",
      "State X: Best action (20, 5) with value 4.08\n",
      "State I: Best action (25, 5) with value 5.20\n",
      "State XI: Best action (25, 5) with value 5.97\n",
      "--- Iteration 1 ---\n",
      "State G: Best action (25, 5) with value 6.70\n",
      "State X: Best action (25, 5) with value 7.98\n",
      "State I: Best action (25, 5) with value 9.08\n",
      "State XI: Best action (25, 5) with value 9.66\n",
      "--- Iteration 2 ---\n",
      "State G: Best action (25, 5) with value 10.10\n",
      "State X: Best action (25, 5) with value 11.29\n",
      "State I: Best action (25, 5) with value 12.22\n",
      "State XI: Best action (25, 5) with value 12.69\n",
      "--- Iteration 3 ---\n",
      "State G: Best action (25, 5) with value 13.06\n",
      "State X: Best action (25, 5) with value 14.09\n",
      "State I: Best action (25, 5) with value 14.88\n",
      "State XI: Best action (25, 5) with value 15.27\n",
      "--- Iteration 4 ---\n",
      "State G: Best action (25, 5) with value 15.58\n",
      "State X: Best action (25, 5) with value 16.46\n",
      "State I: Best action (25, 5) with value 17.13\n",
      "State XI: Best action (25, 5) with value 17.47\n",
      "--- Iteration 5 ---\n",
      "State G: Best action (25, 5) with value 17.73\n",
      "State X: Best action (25, 5) with value 18.48\n",
      "State I: Best action (25, 5) with value 19.05\n",
      "State XI: Best action (25, 5) with value 19.34\n",
      "--- Iteration 6 ---\n",
      "State G: Best action (25, 5) with value 19.56\n",
      "State X: Best action (25, 5) with value 20.20\n",
      "State I: Best action (25, 5) with value 20.68\n",
      "State XI: Best action (25, 5) with value 20.93\n",
      "--- Iteration 7 ---\n",
      "State G: Best action (25, 5) with value 21.12\n",
      "State X: Best action (25, 5) with value 21.66\n",
      "State I: Best action (25, 5) with value 22.07\n",
      "State XI: Best action (25, 5) with value 22.28\n",
      "--- Iteration 8 ---\n",
      "State G: Best action (25, 5) with value 22.45\n",
      "State X: Best action (25, 5) with value 22.91\n",
      "State I: Best action (25, 5) with value 23.26\n",
      "State XI: Best action (25, 5) with value 23.43\n",
      "--- Iteration 9 ---\n",
      "State G: Best action (25, 5) with value 23.57\n",
      "State X: Best action (25, 5) with value 23.97\n",
      "State I: Best action (25, 5) with value 24.26\n",
      "State XI: Best action (25, 5) with value 24.41\n",
      "--- Iteration 10 ---\n",
      "State G: Best action (25, 5) with value 24.53\n",
      "State X: Best action (25, 5) with value 24.87\n",
      "State I: Best action (25, 5) with value 25.12\n",
      "State XI: Best action (25, 5) with value 25.25\n",
      "--- Iteration 11 ---\n",
      "State G: Best action (25, 5) with value 25.35\n",
      "State X: Best action (25, 5) with value 25.63\n",
      "State I: Best action (25, 5) with value 25.85\n",
      "State XI: Best action (25, 5) with value 25.96\n",
      "--- Iteration 12 ---\n",
      "State G: Best action (25, 5) with value 26.05\n",
      "State X: Best action (25, 5) with value 26.29\n",
      "State I: Best action (25, 5) with value 26.47\n",
      "State XI: Best action (25, 5) with value 26.56\n",
      "--- Iteration 13 ---\n",
      "State G: Best action (25, 5) with value 26.64\n",
      "State X: Best action (25, 5) with value 26.84\n",
      "State I: Best action (25, 5) with value 27.00\n",
      "State XI: Best action (25, 5) with value 27.08\n",
      "--- Iteration 14 ---\n",
      "State G: Best action (25, 5) with value 27.14\n",
      "State X: Best action (25, 5) with value 27.32\n",
      "State I: Best action (25, 5) with value 27.45\n",
      "State XI: Best action (25, 5) with value 27.52\n",
      "--- Iteration 15 ---\n",
      "State G: Best action (25, 5) with value 27.57\n",
      "State X: Best action (25, 5) with value 27.72\n",
      "State I: Best action (25, 5) with value 27.83\n",
      "State XI: Best action (25, 5) with value 27.89\n",
      "--- Iteration 16 ---\n",
      "State G: Best action (25, 5) with value 27.93\n",
      "State X: Best action (25, 5) with value 28.06\n",
      "State I: Best action (25, 5) with value 28.16\n",
      "State XI: Best action (25, 5) with value 28.21\n",
      "--- Iteration 17 ---\n",
      "State G: Best action (25, 5) with value 28.24\n",
      "State X: Best action (25, 5) with value 28.35\n",
      "State I: Best action (25, 5) with value 28.43\n",
      "State XI: Best action (25, 5) with value 28.48\n",
      "--- Iteration 18 ---\n",
      "State G: Best action (25, 5) with value 28.51\n",
      "State X: Best action (25, 5) with value 28.60\n",
      "State I: Best action (25, 5) with value 28.67\n",
      "State XI: Best action (25, 5) with value 28.71\n",
      "--- Iteration 19 ---\n",
      "State G: Best action (25, 5) with value 28.73\n",
      "State X: Best action (25, 5) with value 28.81\n",
      "State I: Best action (25, 5) with value 28.87\n",
      "State XI: Best action (25, 5) with value 28.90\n",
      "--- Iteration 20 ---\n",
      "State G: Best action (25, 5) with value 28.92\n",
      "State X: Best action (25, 5) with value 28.99\n",
      "State I: Best action (25, 5) with value 29.04\n",
      "State XI: Best action (25, 5) with value 29.07\n",
      "--- Iteration 21 ---\n",
      "State G: Best action (25, 5) with value 29.09\n",
      "State X: Best action (25, 5) with value 29.14\n",
      "State I: Best action (25, 5) with value 29.19\n",
      "State XI: Best action (25, 5) with value 29.21\n",
      "--- Iteration 22 ---\n",
      "State G: Best action (25, 5) with value 29.23\n",
      "State X: Best action (25, 5) with value 29.27\n",
      "State I: Best action (25, 5) with value 29.31\n",
      "State XI: Best action (25, 5) with value 29.33\n",
      "--- Iteration 23 ---\n",
      "State G: Best action (25, 5) with value 29.34\n",
      "State X: Best action (25, 5) with value 29.39\n",
      "State I: Best action (25, 5) with value 29.42\n",
      "State XI: Best action (25, 5) with value 29.43\n",
      "--- Iteration 24 ---\n",
      "State G: Best action (25, 5) with value 29.44\n",
      "State X: Best action (25, 5) with value 29.48\n",
      "State I: Best action (25, 5) with value 29.51\n",
      "State XI: Best action (25, 5) with value 29.52\n",
      "--- Iteration 25 ---\n",
      "State G: Best action (25, 5) with value 29.53\n",
      "State X: Best action (25, 5) with value 29.56\n",
      "State I: Best action (25, 5) with value 29.58\n",
      "State XI: Best action (25, 5) with value 29.59\n",
      "--- Iteration 26 ---\n",
      "State G: Best action (25, 5) with value 29.60\n",
      "State X: Best action (25, 5) with value 29.63\n",
      "State I: Best action (25, 5) with value 29.65\n",
      "State XI: Best action (25, 5) with value 29.66\n",
      "--- Iteration 27 ---\n",
      "State G: Best action (25, 5) with value 29.66\n",
      "State X: Best action (25, 5) with value 29.69\n",
      "State I: Best action (25, 5) with value 29.70\n",
      "State XI: Best action (25, 5) with value 29.71\n",
      "--- Iteration 28 ---\n",
      "State G: Best action (25, 5) with value 29.72\n",
      "State X: Best action (25, 5) with value 29.74\n",
      "State I: Best action (25, 5) with value 29.75\n",
      "State XI: Best action (25, 5) with value 29.76\n",
      "--- Iteration 29 ---\n",
      "State G: Best action (25, 5) with value 29.76\n",
      "State X: Best action (25, 5) with value 29.78\n",
      "State I: Best action (25, 5) with value 29.79\n",
      "State XI: Best action (25, 5) with value 29.80\n",
      "--- Iteration 30 ---\n",
      "State G: Best action (25, 5) with value 29.80\n",
      "State X: Best action (25, 5) with value 29.81\n",
      "State I: Best action (25, 5) with value 29.82\n",
      "State XI: Best action (25, 5) with value 29.83\n",
      "--- Iteration 31 ---\n",
      "State G: Best action (25, 5) with value 29.83\n",
      "State X: Best action (25, 5) with value 29.84\n",
      "State I: Best action (25, 5) with value 29.85\n",
      "State XI: Best action (25, 5) with value 29.86\n",
      "--- Iteration 32 ---\n",
      "State G: Best action (25, 5) with value 29.86\n",
      "State X: Best action (25, 5) with value 29.87\n",
      "State I: Best action (25, 5) with value 29.88\n",
      "State XI: Best action (25, 5) with value 29.88\n",
      "--- Iteration 33 ---\n",
      "State G: Best action (25, 5) with value 29.88\n",
      "State X: Best action (25, 5) with value 29.89\n",
      "State I: Best action (25, 5) with value 29.90\n",
      "State XI: Best action (25, 5) with value 29.90\n",
      "--- Iteration 34 ---\n",
      "State G: Best action (25, 5) with value 29.90\n",
      "State X: Best action (25, 5) with value 29.91\n",
      "State I: Best action (25, 5) with value 29.92\n",
      "State XI: Best action (25, 5) with value 29.92\n",
      "--- Iteration 35 ---\n",
      "State G: Best action (25, 5) with value 29.92\n",
      "State X: Best action (25, 5) with value 29.93\n",
      "State I: Best action (25, 5) with value 29.93\n",
      "State XI: Best action (25, 5) with value 29.93\n",
      "--- Iteration 36 ---\n",
      "State G: Best action (25, 5) with value 29.94\n",
      "State X: Best action (25, 5) with value 29.94\n",
      "State I: Best action (25, 5) with value 29.94\n",
      "State XI: Best action (25, 5) with value 29.95\n",
      "--- Iteration 37 ---\n",
      "State G: Best action (25, 5) with value 29.95\n",
      "State X: Best action (25, 5) with value 29.95\n",
      "State I: Best action (25, 5) with value 29.95\n",
      "State XI: Best action (25, 5) with value 29.96\n",
      "--- Iteration 38 ---\n",
      "State G: Best action (25, 5) with value 29.96\n",
      "State X: Best action (25, 5) with value 29.96\n",
      "State I: Best action (25, 5) with value 29.96\n",
      "State XI: Best action (25, 5) with value 29.97\n",
      "--- Iteration 39 ---\n",
      "State G: Best action (25, 5) with value 29.97\n",
      "State X: Best action (25, 5) with value 29.97\n",
      "State I: Best action (25, 5) with value 29.97\n",
      "State XI: Best action (25, 5) with value 29.97\n",
      "--- Iteration 40 ---\n",
      "State G: Best action (25, 5) with value 29.97\n",
      "State X: Best action (25, 5) with value 29.98\n",
      "State I: Best action (25, 5) with value 29.98\n",
      "State XI: Best action (25, 5) with value 29.98\n",
      "--- Iteration 41 ---\n",
      "State G: Best action (25, 5) with value 29.98\n",
      "State X: Best action (25, 5) with value 29.98\n",
      "State I: Best action (25, 5) with value 29.98\n",
      "State XI: Best action (25, 5) with value 29.99\n",
      "--- Iteration 42 ---\n",
      "State G: Best action (25, 5) with value 29.99\n",
      "State X: Best action (25, 5) with value 29.99\n",
      "State I: Best action (25, 5) with value 29.99\n",
      "State XI: Best action (25, 5) with value 29.99\n",
      "--- Iteration 43 ---\n",
      "State G: Best action (25, 5) with value 29.99\n",
      "State X: Best action (25, 5) with value 29.99\n",
      "State I: Best action (25, 5) with value 29.99\n",
      "State XI: Best action (25, 5) with value 29.99\n",
      "--- Iteration 44 ---\n",
      "State G: Best action (25, 5) with value 30.00\n",
      "State X: Best action (25, 5) with value 30.00\n",
      "State I: Best action (25, 5) with value 30.00\n",
      "State XI: Best action (25, 5) with value 30.00\n",
      "--- Iteration 45 ---\n",
      "State G: Best action (25, 5) with value 30.00\n",
      "State X: Best action (25, 5) with value 30.00\n",
      "State I: Best action (25, 5) with value 30.00\n",
      "State XI: Best action (25, 5) with value 30.00\n",
      "--- Iteration 46 ---\n",
      "State G: Best action (25, 5) with value 30.00\n",
      "State X: Best action (25, 5) with value 30.00\n",
      "State I: Best action (25, 5) with value 30.00\n",
      "State XI: Best action (25, 5) with value 30.00\n",
      "--- Iteration 47 ---\n",
      "State G: Best action (25, 5) with value 30.00\n",
      "State X: Best action (25, 5) with value 30.00\n",
      "State I: Best action (25, 5) with value 30.01\n",
      "State XI: Best action (25, 5) with value 30.01\n",
      "--- Iteration 48 ---\n",
      "State G: Best action (25, 5) with value 30.01\n",
      "State X: Best action (25, 5) with value 30.01\n",
      "State I: Best action (25, 5) with value 30.01\n",
      "State XI: Best action (25, 5) with value 30.01\n",
      "--- Iteration 49 ---\n",
      "State G: Best action (25, 5) with value 30.01\n",
      "State X: Best action (25, 5) with value 30.01\n",
      "State I: Best action (25, 5) with value 30.01\n",
      "State XI: Best action (25, 5) with value 30.01\n",
      "--- Iteration 50 ---\n",
      "State G: Best action (25, 5) with value 30.01\n",
      "State X: Best action (25, 5) with value 30.01\n",
      "State I: Best action (25, 5) with value 30.01\n",
      "State XI: Best action (25, 5) with value 30.01\n",
      "--- Iteration 51 ---\n",
      "State G: Best action (25, 5) with value 30.01\n",
      "State X: Best action (25, 5) with value 30.01\n",
      "State I: Best action (25, 5) with value 30.01\n",
      "State XI: Best action (25, 5) with value 30.01\n",
      "--- Iteration 52 ---\n",
      "State G: Best action (25, 5) with value 30.01\n",
      "State X: Best action (25, 5) with value 30.01\n",
      "State I: Best action (25, 5) with value 30.01\n",
      "State XI: Best action (25, 5) with value 30.01\n",
      "--- Iteration 53 ---\n",
      "State G: Best action (25, 5) with value 30.01\n",
      "State X: Best action (25, 5) with value 30.01\n",
      "State I: Best action (25, 5) with value 30.01\n",
      "State XI: Best action (25, 5) with value 30.01\n",
      "--- Iteration 54 ---\n",
      "State G: Best action (25, 5) with value 30.01\n",
      "State X: Best action (25, 5) with value 30.01\n",
      "State I: Best action (25, 5) with value 30.01\n",
      "State XI: Best action (25, 5) with value 30.01\n",
      "--- Iteration 55 ---\n",
      "State G: Best action (25, 5) with value 30.01\n",
      "State X: Best action (25, 5) with value 30.01\n",
      "State I: Best action (25, 5) with value 30.01\n",
      "State XI: Best action (25, 5) with value 30.01\n",
      "--- Iteration 56 ---\n",
      "State G: Best action (25, 5) with value 30.01\n",
      "State X: Best action (25, 5) with value 30.01\n",
      "State I: Best action (25, 5) with value 30.01\n",
      "State XI: Best action (25, 5) with value 30.01\n",
      "--- Iteration 57 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "--- Iteration 58 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "--- Iteration 59 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "--- Iteration 60 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "--- Iteration 61 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "--- Iteration 62 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "--- Iteration 63 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "--- Iteration 64 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "--- Iteration 65 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "--- Iteration 66 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "--- Iteration 67 ---\n",
      "State G: Best action (25, 5) with value 30.02\n",
      "State X: Best action (25, 5) with value 30.02\n",
      "State I: Best action (25, 5) with value 30.02\n",
      "State XI: Best action (25, 5) with value 30.02\n",
      "Convergence reached.\n",
      "\n",
      "Final Value Function:\n",
      "G: 30.02\n",
      "X: 30.02\n",
      "I: 30.02\n",
      "XI: 30.02\n",
      "\n",
      "Final Policy:\n",
      "G: (25, 5)\n",
      "X: (25, 5)\n",
      "I: (25, 5)\n",
      "XI: (25, 5)\n"
     ]
    }
   ],
   "source": [
    "# --- Value Iteration ---\n",
    "def value_iteration(states, actions, transition_func, reward_func, environment, gamma=0.9, theta=1e-4):\n",
    "    \"\"\"\n",
    "    Perform value iteration on a fixed Constrained Markov Decision Process (CMDP).\n",
    "\n",
    "    Parameters:\n",
    "    - states: List of states in the CMDP.\n",
    "    - actions: List of available actions for each state.\n",
    "    - transition_func: Function that returns transition probabilities for a given state-action pair.\n",
    "    - reward_func: Function that calculates the reward for a state-action-next state combination.\n",
    "    - environment: The CMDP environment with precomputed transition and reward matrices.\n",
    "    - gamma: Discount factor for future rewards (default 0.9).\n",
    "    - theta: Convergence threshold for stopping iteration (default 1e-4).\n",
    "\n",
    "    Returns:\n",
    "    - V: Final value function for each state.\n",
    "    - policy: Optimal policy indicating the best action for each state.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize V(s) = 0 for all states and empty policy\n",
    "    V = {s: 0 for s in states}\n",
    "    policy = {s: None for s in states}\n",
    "\n",
    "    # Use fixed transition and reward matrices computed from the environment\n",
    "    T = transitionProbabilitiesMatrix(actions, environment)\n",
    "    # Reward components for different conditions in the environment\n",
    "    r1, r2, r3, r4 = 5, 10, 0, -1\n",
    "\n",
    "    R = expectedRewardMatrix(actions, environment, r1, r2, r3, r4)\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        print(f\"--- Iteration {iteration} ---\")\n",
    "        iteration += 1\n",
    "\n",
    "        # iterate over each state to update its value and determine the best action.\n",
    "        for s in states:\n",
    "            v_old = V[s] # Store the current value for convergence check (change in value)\n",
    "            action_values = {} # Store the expected value for each action in this state\n",
    "            \n",
    "            # Calculate the expected value for each action in the current state\n",
    "            for a in actions: \n",
    "                expected_value = 0\n",
    "\n",
    "                # Iterate over all possible next states s' and their transition probabilities p.\n",
    "                for s_prime, p in transition_func(s, a, T):\n",
    "                    expected_value += p * (reward_func(s, a, s_prime, R) + gamma * V[s_prime]) # Bellman optimally equation\n",
    "                    # sums the reward for transitioning to s' from s with action a and the discounted value of s' (future value).\n",
    "                action_values[a] = expected_value # Store the expected value for this action\n",
    "\n",
    "            # Select the best action and update the value function\n",
    "            best_action_value = max(action_values.values())\n",
    "            # The action that provides the maximum expected value\n",
    "            best_action = max(action_values, key=action_values.get)\n",
    "            # Update the value function and policy with the best action\n",
    "            V[s] = best_action_value\n",
    "            policy[s] = best_action\n",
    "            print(f\"State {s}: Best action {best_action} with value {best_action_value:.2f}\")\n",
    "\n",
    "            # Update delta to be the maximum change across states for convergence check\n",
    "            delta = max(delta, abs(v_old - V[s]))\n",
    "            \n",
    "         # Convergence condition: If the maximum change is below the threshold theta\n",
    "        if delta < theta:\n",
    "            print(\"Convergence reached.\")\n",
    "            break\n",
    "    return V, policy\n",
    "\n",
    "# Run value iteration on the fixed model\n",
    "V, policy = value_iteration(states, actions, transition_func, reward_func, kinematicModel, gamma=0.9, theta=1e-4)\n",
    "\n",
    "print(\"\\nFinal Value Function:\")\n",
    "for state, value in V.items():\n",
    "    print(f\"{state}: {value:.2f}\")\n",
    "\n",
    "print(\"\\nFinal Policy:\")\n",
    "for state, action in policy.items():\n",
    "    print(f\"{state}: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53d040bf-e16f-4e3c-ad3c-ebcc9d3a7b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 0 ---\n",
      "State G: Best action (20, 5) with value -10.06\n",
      "State X: Best action (20, 5) with value -16.93\n",
      "State I: Best action (20, 5) with value -20.62\n",
      "State XI: Best action (20, 5) with value -20.62\n",
      "--- Iteration 1 ---\n",
      "State G: Best action (20, 5) with value -20.62\n",
      "State X: Best action (20, 5) with value -27.81\n",
      "State I: Best action (20, 5) with value -30.18\n",
      "State XI: Best action (20, 5) with value -30.18\n",
      "--- Iteration 2 ---\n",
      "State G: Best action (20, 5) with value -30.18\n",
      "State X: Best action (20, 5) with value -36.71\n",
      "State I: Best action (20, 5) with value -38.65\n",
      "State XI: Best action (20, 5) with value -38.65\n",
      "--- Iteration 3 ---\n",
      "State G: Best action (20, 5) with value -38.65\n",
      "State X: Best action (20, 5) with value -44.43\n",
      "State I: Best action (20, 5) with value -46.11\n",
      "State XI: Best action (20, 5) with value -46.11\n",
      "--- Iteration 4 ---\n",
      "State G: Best action (20, 5) with value -46.11\n",
      "State X: Best action (20, 5) with value -51.19\n",
      "State I: Best action (20, 5) with value -52.67\n",
      "State XI: Best action (20, 5) with value -52.67\n",
      "--- Iteration 5 ---\n",
      "State G: Best action (20, 5) with value -52.67\n",
      "State X: Best action (20, 5) with value -57.14\n",
      "State I: Best action (20, 5) with value -58.44\n",
      "State XI: Best action (20, 5) with value -58.44\n",
      "--- Iteration 6 ---\n",
      "State G: Best action (20, 5) with value -58.44\n",
      "State X: Best action (20, 5) with value -62.38\n",
      "State I: Best action (20, 5) with value -63.51\n",
      "State XI: Best action (20, 5) with value -63.51\n",
      "--- Iteration 7 ---\n",
      "State G: Best action (20, 5) with value -63.51\n",
      "State X: Best action (20, 5) with value -66.98\n",
      "State I: Best action (20, 5) with value -67.98\n",
      "State XI: Best action (20, 5) with value -67.98\n",
      "--- Iteration 8 ---\n",
      "State G: Best action (20, 5) with value -67.98\n",
      "State X: Best action (20, 5) with value -71.03\n",
      "State I: Best action (20, 5) with value -71.91\n",
      "State XI: Best action (20, 5) with value -71.91\n",
      "--- Iteration 9 ---\n",
      "State G: Best action (20, 5) with value -71.91\n",
      "State X: Best action (20, 5) with value -74.59\n",
      "State I: Best action (20, 5) with value -75.37\n",
      "State XI: Best action (20, 5) with value -75.37\n",
      "--- Iteration 10 ---\n",
      "State G: Best action (20, 5) with value -75.37\n",
      "State X: Best action (20, 5) with value -77.72\n",
      "State I: Best action (20, 5) with value -78.41\n",
      "State XI: Best action (20, 5) with value -78.41\n",
      "--- Iteration 11 ---\n",
      "State G: Best action (20, 5) with value -78.41\n",
      "State X: Best action (20, 5) with value -80.48\n",
      "State I: Best action (20, 5) with value -81.08\n",
      "State XI: Best action (20, 5) with value -81.08\n",
      "--- Iteration 12 ---\n",
      "State G: Best action (20, 5) with value -81.08\n",
      "State X: Best action (20, 5) with value -82.90\n",
      "State I: Best action (20, 5) with value -83.43\n",
      "State XI: Best action (20, 5) with value -83.43\n",
      "--- Iteration 13 ---\n",
      "State G: Best action (20, 5) with value -83.43\n",
      "State X: Best action (20, 5) with value -85.04\n",
      "State I: Best action (20, 5) with value -85.50\n",
      "State XI: Best action (20, 5) with value -85.50\n",
      "--- Iteration 14 ---\n",
      "State G: Best action (20, 5) with value -85.50\n",
      "State X: Best action (20, 5) with value -86.91\n",
      "State I: Best action (20, 5) with value -87.32\n",
      "State XI: Best action (20, 5) with value -87.32\n",
      "--- Iteration 15 ---\n",
      "State G: Best action (20, 5) with value -87.32\n",
      "State X: Best action (20, 5) with value -88.56\n",
      "State I: Best action (20, 5) with value -88.92\n",
      "State XI: Best action (20, 5) with value -88.92\n",
      "--- Iteration 16 ---\n",
      "State G: Best action (20, 5) with value -88.92\n",
      "State X: Best action (20, 5) with value -90.02\n",
      "State I: Best action (20, 5) with value -90.33\n",
      "State XI: Best action (20, 5) with value -90.33\n",
      "--- Iteration 17 ---\n",
      "State G: Best action (20, 5) with value -90.33\n",
      "State X: Best action (20, 5) with value -91.29\n",
      "State I: Best action (20, 5) with value -91.57\n",
      "State XI: Best action (20, 5) with value -91.57\n",
      "--- Iteration 18 ---\n",
      "State G: Best action (20, 5) with value -91.57\n",
      "State X: Best action (20, 5) with value -92.42\n",
      "State I: Best action (20, 5) with value -92.66\n",
      "State XI: Best action (20, 5) with value -92.66\n",
      "--- Iteration 19 ---\n",
      "State G: Best action (20, 5) with value -92.66\n",
      "State X: Best action (20, 5) with value -93.41\n",
      "State I: Best action (20, 5) with value -93.62\n",
      "State XI: Best action (20, 5) with value -93.62\n",
      "--- Iteration 20 ---\n",
      "State G: Best action (20, 5) with value -93.62\n",
      "State X: Best action (20, 5) with value -94.27\n",
      "State I: Best action (20, 5) with value -94.46\n",
      "State XI: Best action (20, 5) with value -94.46\n",
      "--- Iteration 21 ---\n",
      "State G: Best action (20, 5) with value -94.46\n",
      "State X: Best action (20, 5) with value -95.04\n",
      "State I: Best action (20, 5) with value -95.21\n",
      "State XI: Best action (20, 5) with value -95.21\n",
      "--- Iteration 22 ---\n",
      "State G: Best action (20, 5) with value -95.21\n",
      "State X: Best action (20, 5) with value -95.71\n",
      "State I: Best action (20, 5) with value -95.86\n",
      "State XI: Best action (20, 5) with value -95.86\n",
      "--- Iteration 23 ---\n",
      "State G: Best action (20, 5) with value -95.86\n",
      "State X: Best action (20, 5) with value -96.30\n",
      "State I: Best action (20, 5) with value -96.43\n",
      "State XI: Best action (20, 5) with value -96.43\n",
      "--- Iteration 24 ---\n",
      "State G: Best action (20, 5) with value -96.43\n",
      "State X: Best action (20, 5) with value -96.82\n",
      "State I: Best action (20, 5) with value -96.94\n",
      "State XI: Best action (20, 5) with value -96.94\n",
      "--- Iteration 25 ---\n",
      "State G: Best action (20, 5) with value -96.94\n",
      "State X: Best action (20, 5) with value -97.28\n",
      "State I: Best action (20, 5) with value -97.38\n",
      "State XI: Best action (20, 5) with value -97.38\n",
      "--- Iteration 26 ---\n",
      "State G: Best action (20, 5) with value -97.38\n",
      "State X: Best action (20, 5) with value -97.69\n",
      "State I: Best action (20, 5) with value -97.77\n",
      "State XI: Best action (20, 5) with value -97.77\n",
      "--- Iteration 27 ---\n",
      "State G: Best action (20, 5) with value -97.77\n",
      "State X: Best action (20, 5) with value -98.04\n",
      "State I: Best action (20, 5) with value -98.12\n",
      "State XI: Best action (20, 5) with value -98.12\n",
      "--- Iteration 28 ---\n",
      "State G: Best action (20, 5) with value -98.12\n",
      "State X: Best action (20, 5) with value -98.35\n",
      "State I: Best action (20, 5) with value -98.42\n",
      "State XI: Best action (20, 5) with value -98.42\n",
      "--- Iteration 29 ---\n",
      "State G: Best action (20, 5) with value -98.42\n",
      "State X: Best action (20, 5) with value -98.63\n",
      "State I: Best action (20, 5) with value -98.69\n",
      "State XI: Best action (20, 5) with value -98.69\n",
      "--- Iteration 30 ---\n",
      "State G: Best action (20, 5) with value -98.69\n",
      "State X: Best action (20, 5) with value -98.87\n",
      "State I: Best action (20, 5) with value -98.92\n",
      "State XI: Best action (20, 5) with value -98.92\n",
      "--- Iteration 31 ---\n",
      "State G: Best action (20, 5) with value -98.92\n",
      "State X: Best action (20, 5) with value -99.08\n",
      "State I: Best action (20, 5) with value -99.13\n",
      "State XI: Best action (20, 5) with value -99.13\n",
      "--- Iteration 32 ---\n",
      "State G: Best action (20, 5) with value -99.13\n",
      "State X: Best action (20, 5) with value -99.27\n",
      "State I: Best action (20, 5) with value -99.31\n",
      "State XI: Best action (20, 5) with value -99.31\n",
      "--- Iteration 33 ---\n",
      "State G: Best action (20, 5) with value -99.31\n",
      "State X: Best action (20, 5) with value -99.43\n",
      "State I: Best action (20, 5) with value -99.47\n",
      "State XI: Best action (20, 5) with value -99.47\n",
      "--- Iteration 34 ---\n",
      "State G: Best action (20, 5) with value -99.47\n",
      "State X: Best action (20, 5) with value -99.58\n",
      "State I: Best action (20, 5) with value -99.61\n",
      "State XI: Best action (20, 5) with value -99.61\n",
      "--- Iteration 35 ---\n",
      "State G: Best action (20, 5) with value -99.61\n",
      "State X: Best action (20, 5) with value -99.70\n",
      "State I: Best action (20, 5) with value -99.73\n",
      "State XI: Best action (20, 5) with value -99.73\n",
      "--- Iteration 36 ---\n",
      "State G: Best action (20, 5) with value -99.73\n",
      "State X: Best action (20, 5) with value -99.81\n",
      "State I: Best action (20, 5) with value -99.84\n",
      "State XI: Best action (20, 5) with value -99.84\n",
      "--- Iteration 37 ---\n",
      "State G: Best action (20, 5) with value -99.84\n",
      "State X: Best action (20, 5) with value -99.91\n",
      "State I: Best action (20, 5) with value -99.93\n",
      "State XI: Best action (20, 5) with value -99.93\n",
      "--- Iteration 38 ---\n",
      "State G: Best action (20, 5) with value -99.93\n",
      "State X: Best action (20, 5) with value -100.00\n",
      "State I: Best action (20, 5) with value -100.02\n",
      "State XI: Best action (20, 5) with value -100.02\n",
      "--- Iteration 39 ---\n",
      "State G: Best action (20, 5) with value -100.02\n",
      "State X: Best action (20, 5) with value -100.08\n",
      "State I: Best action (20, 5) with value -100.09\n",
      "State XI: Best action (20, 5) with value -100.09\n",
      "--- Iteration 40 ---\n",
      "State G: Best action (20, 5) with value -100.09\n",
      "State X: Best action (20, 5) with value -100.14\n",
      "State I: Best action (20, 5) with value -100.16\n",
      "State XI: Best action (20, 5) with value -100.16\n",
      "--- Iteration 41 ---\n",
      "State G: Best action (20, 5) with value -100.16\n",
      "State X: Best action (20, 5) with value -100.20\n",
      "State I: Best action (20, 5) with value -100.21\n",
      "State XI: Best action (20, 5) with value -100.21\n",
      "--- Iteration 42 ---\n",
      "State G: Best action (20, 5) with value -100.21\n",
      "State X: Best action (20, 5) with value -100.25\n",
      "State I: Best action (20, 5) with value -100.26\n",
      "State XI: Best action (20, 5) with value -100.26\n",
      "--- Iteration 43 ---\n",
      "State G: Best action (20, 5) with value -100.26\n",
      "State X: Best action (20, 5) with value -100.30\n",
      "State I: Best action (20, 5) with value -100.31\n",
      "State XI: Best action (20, 5) with value -100.31\n",
      "--- Iteration 44 ---\n",
      "State G: Best action (20, 5) with value -100.31\n",
      "State X: Best action (20, 5) with value -100.34\n",
      "State I: Best action (20, 5) with value -100.35\n",
      "State XI: Best action (20, 5) with value -100.35\n",
      "--- Iteration 45 ---\n",
      "State G: Best action (20, 5) with value -100.35\n",
      "State X: Best action (20, 5) with value -100.37\n",
      "State I: Best action (20, 5) with value -100.38\n",
      "State XI: Best action (20, 5) with value -100.38\n",
      "--- Iteration 46 ---\n",
      "State G: Best action (20, 5) with value -100.38\n",
      "State X: Best action (20, 5) with value -100.40\n",
      "State I: Best action (20, 5) with value -100.41\n",
      "State XI: Best action (20, 5) with value -100.41\n",
      "--- Iteration 47 ---\n",
      "State G: Best action (20, 5) with value -100.41\n",
      "State X: Best action (20, 5) with value -100.43\n",
      "State I: Best action (20, 5) with value -100.44\n",
      "State XI: Best action (20, 5) with value -100.44\n",
      "--- Iteration 48 ---\n",
      "State G: Best action (20, 5) with value -100.44\n",
      "State X: Best action (20, 5) with value -100.46\n",
      "State I: Best action (20, 5) with value -100.46\n",
      "State XI: Best action (20, 5) with value -100.46\n",
      "--- Iteration 49 ---\n",
      "State G: Best action (20, 5) with value -100.46\n",
      "State X: Best action (20, 5) with value -100.48\n",
      "State I: Best action (20, 5) with value -100.48\n",
      "State XI: Best action (20, 5) with value -100.48\n",
      "--- Iteration 50 ---\n",
      "State G: Best action (20, 5) with value -100.48\n",
      "State X: Best action (20, 5) with value -100.50\n",
      "State I: Best action (20, 5) with value -100.50\n",
      "State XI: Best action (20, 5) with value -100.50\n",
      "--- Iteration 51 ---\n",
      "State G: Best action (20, 5) with value -100.50\n",
      "State X: Best action (20, 5) with value -100.51\n",
      "State I: Best action (20, 5) with value -100.52\n",
      "State XI: Best action (20, 5) with value -100.52\n",
      "--- Iteration 52 ---\n",
      "State G: Best action (20, 5) with value -100.52\n",
      "State X: Best action (20, 5) with value -100.53\n",
      "State I: Best action (20, 5) with value -100.53\n",
      "State XI: Best action (20, 5) with value -100.53\n",
      "--- Iteration 53 ---\n",
      "State G: Best action (20, 5) with value -100.53\n",
      "State X: Best action (20, 5) with value -100.54\n",
      "State I: Best action (20, 5) with value -100.54\n",
      "State XI: Best action (20, 5) with value -100.54\n",
      "--- Iteration 54 ---\n",
      "State G: Best action (20, 5) with value -100.54\n",
      "State X: Best action (20, 5) with value -100.55\n",
      "State I: Best action (20, 5) with value -100.55\n",
      "State XI: Best action (20, 5) with value -100.55\n",
      "--- Iteration 55 ---\n",
      "State G: Best action (20, 5) with value -100.55\n",
      "State X: Best action (20, 5) with value -100.56\n",
      "State I: Best action (20, 5) with value -100.56\n",
      "State XI: Best action (20, 5) with value -100.56\n",
      "--- Iteration 56 ---\n",
      "State G: Best action (20, 5) with value -100.56\n",
      "State X: Best action (20, 5) with value -100.57\n",
      "State I: Best action (20, 5) with value -100.57\n",
      "State XI: Best action (20, 5) with value -100.57\n",
      "--- Iteration 57 ---\n",
      "State G: Best action (20, 5) with value -100.57\n",
      "State X: Best action (20, 5) with value -100.58\n",
      "State I: Best action (20, 5) with value -100.58\n",
      "State XI: Best action (20, 5) with value -100.58\n",
      "--- Iteration 58 ---\n",
      "State G: Best action (20, 5) with value -100.58\n",
      "State X: Best action (20, 5) with value -100.58\n",
      "State I: Best action (20, 5) with value -100.58\n",
      "State XI: Best action (20, 5) with value -100.58\n",
      "--- Iteration 59 ---\n",
      "State G: Best action (20, 5) with value -100.58\n",
      "State X: Best action (20, 5) with value -100.59\n",
      "State I: Best action (20, 5) with value -100.59\n",
      "State XI: Best action (20, 5) with value -100.59\n",
      "--- Iteration 60 ---\n",
      "State G: Best action (20, 5) with value -100.59\n",
      "State X: Best action (20, 5) with value -100.59\n",
      "State I: Best action (20, 5) with value -100.59\n",
      "State XI: Best action (20, 5) with value -100.59\n",
      "--- Iteration 61 ---\n",
      "State G: Best action (20, 5) with value -100.59\n",
      "State X: Best action (20, 5) with value -100.60\n",
      "State I: Best action (20, 5) with value -100.60\n",
      "State XI: Best action (20, 5) with value -100.60\n",
      "--- Iteration 62 ---\n",
      "State G: Best action (20, 5) with value -100.60\n",
      "State X: Best action (20, 5) with value -100.60\n",
      "State I: Best action (20, 5) with value -100.60\n",
      "State XI: Best action (20, 5) with value -100.60\n",
      "--- Iteration 63 ---\n",
      "State G: Best action (20, 5) with value -100.60\n",
      "State X: Best action (20, 5) with value -100.61\n",
      "State I: Best action (20, 5) with value -100.61\n",
      "State XI: Best action (20, 5) with value -100.61\n",
      "--- Iteration 64 ---\n",
      "State G: Best action (20, 5) with value -100.61\n",
      "State X: Best action (20, 5) with value -100.61\n",
      "State I: Best action (20, 5) with value -100.61\n",
      "State XI: Best action (20, 5) with value -100.61\n",
      "--- Iteration 65 ---\n",
      "State G: Best action (20, 5) with value -100.61\n",
      "State X: Best action (20, 5) with value -100.61\n",
      "State I: Best action (20, 5) with value -100.61\n",
      "State XI: Best action (20, 5) with value -100.61\n",
      "--- Iteration 66 ---\n",
      "State G: Best action (20, 5) with value -100.61\n",
      "State X: Best action (20, 5) with value -100.61\n",
      "State I: Best action (20, 5) with value -100.61\n",
      "State XI: Best action (20, 5) with value -100.61\n",
      "--- Iteration 67 ---\n",
      "State G: Best action (20, 5) with value -100.61\n",
      "State X: Best action (20, 5) with value -100.62\n",
      "State I: Best action (20, 5) with value -100.62\n",
      "State XI: Best action (20, 5) with value -100.62\n",
      "--- Iteration 68 ---\n",
      "State G: Best action (20, 5) with value -100.62\n",
      "State X: Best action (20, 5) with value -100.62\n",
      "State I: Best action (20, 5) with value -100.62\n",
      "State XI: Best action (20, 5) with value -100.62\n",
      "--- Iteration 69 ---\n",
      "State G: Best action (20, 5) with value -100.62\n",
      "State X: Best action (20, 5) with value -100.62\n",
      "State I: Best action (20, 5) with value -100.62\n",
      "State XI: Best action (20, 5) with value -100.62\n",
      "--- Iteration 70 ---\n",
      "State G: Best action (20, 5) with value -100.62\n",
      "State X: Best action (20, 5) with value -100.62\n",
      "State I: Best action (20, 5) with value -100.62\n",
      "State XI: Best action (20, 5) with value -100.62\n",
      "--- Iteration 71 ---\n",
      "State G: Best action (20, 5) with value -100.62\n",
      "State X: Best action (20, 5) with value -100.62\n",
      "State I: Best action (20, 5) with value -100.62\n",
      "State XI: Best action (20, 5) with value -100.62\n",
      "--- Iteration 72 ---\n",
      "State G: Best action (20, 5) with value -100.62\n",
      "State X: Best action (20, 5) with value -100.62\n",
      "State I: Best action (20, 5) with value -100.62\n",
      "State XI: Best action (20, 5) with value -100.62\n",
      "--- Iteration 73 ---\n",
      "State G: Best action (20, 5) with value -100.62\n",
      "State X: Best action (20, 5) with value -100.62\n",
      "State I: Best action (20, 5) with value -100.62\n",
      "State XI: Best action (20, 5) with value -100.62\n",
      "--- Iteration 74 ---\n",
      "State G: Best action (20, 5) with value -100.62\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 75 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 76 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 77 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 78 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 79 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 80 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 81 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 82 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 83 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 84 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 85 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 86 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 87 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 88 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 89 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 90 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 91 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "--- Iteration 92 ---\n",
      "State G: Best action (20, 5) with value -100.63\n",
      "State X: Best action (20, 5) with value -100.63\n",
      "State I: Best action (20, 5) with value -100.63\n",
      "State XI: Best action (20, 5) with value -100.63\n",
      "Convergence reached.\n",
      "\n",
      "Final Value Function:\n",
      "G: -100.63\n",
      "X: -100.63\n",
      "I: -100.63\n",
      "XI: -100.63\n",
      "\n",
      "Final Policy:\n",
      "G: (20, 5)\n",
      "X: (20, 5)\n",
      "I: (20, 5)\n",
      "XI: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "# Run value iteration on the fixed model\n",
    "V, policy = value_iteration(states, actions, transition_func, reward_func_SAFE, kinematicModel, gamma=0.9, theta=1e-4)\n",
    "\n",
    "print(\"\\nFinal Value Function:\")\n",
    "for state, value in V.items():\n",
    "    print(f\"{state}: {value:.2f}\")\n",
    "\n",
    "print(\"\\nFinal Policy:\")\n",
    "for state, action in policy.items():\n",
    "    print(f\"{state}: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f5f44-82e1-40e7-ba1a-5e2c73dc37a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safeRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
